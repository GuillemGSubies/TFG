{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fill_with(matrix, thing):\n",
    "    \"\"\"\n",
    "    Fills list of lists with 'thing' from the left so its matrix-like with no empty spaces\n",
    "    :param matrix: The list of lists\n",
    "    :param thing: What we fill with\n",
    "    :return: The matrix with all the rows the same len\n",
    "    \"\"\"\n",
    "    length = max([len(i) for i in matrix])\n",
    "    return [[thing] * (length - len(i)) + i for i in matrix]\n",
    "\n",
    "\n",
    "def one_hot_encode(x, y, tokens):\n",
    "    \"\"\"\n",
    "    One-hot encodes x and y\n",
    "    :param x: The x element to One-hot encode\n",
    "    :param y: The target to One-hot encode\n",
    "    :param tokens: The token list so we can encode\n",
    "    :return: x and y One-hot encoded\n",
    "    \"\"\"\n",
    "    # we know len(x[0]) = 2*max_len-1\n",
    "    oh_x = np.zeros((len(x), len(x[0]), len(tokens)), dtype=np.bool)\n",
    "    oh_y = np.zeros((len(x), len(tokens)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(x):\n",
    "        for j, char in enumerate(sentence):\n",
    "            oh_x[i, j, char] = 1\n",
    "        oh_y[i, y[i]] = 1\n",
    "    return oh_x, oh_y\n",
    "\n",
    "\n",
    "def create_dataset(max_len=30, n_of_sentences=50, n=1):\n",
    "    \"\"\"\n",
    "    Prepares a dataset so we can use it in a NN\n",
    "    :param max_len: The maximum length of the sentences\n",
    "    :param n_of_sentences: The number of sencences\n",
    "    :param n: The file i open xD\n",
    "    :return: x y and the tokens\n",
    "    \"\"\"\n",
    "    if n is 1:\n",
    "        fail = open(\"Files/lorem2.txt\", \"r+\")\n",
    "    else:\n",
    "        fail = open(\"Files/lorem3.txt\", \"r+\")\n",
    "    list_lines = fail.readlines()\n",
    "    fail.close()\n",
    "    # We always get the same encoding this way\n",
    "    tokens = sorted(list(set(\"\".join(list_lines))))\n",
    "    if \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    tokens.insert(0, \"Null\")\n",
    "    # print(\"Total tokens: \" + str(len(tokens)))\n",
    "    # print(\"Tokens \" + str(tokens))\n",
    "    char2index = dict((c, i) for i, c in enumerate(tokens))\n",
    "    # print(\"char2index \" + str(char2index))\n",
    "    # index2char = dict((i, c) for i, c in enumerate(tokens))\n",
    "    count = 0\n",
    "    final_y = []\n",
    "    x = []\n",
    "    for line in list_lines:\n",
    "        if (line != \"\\n\") and (count < n_of_sentences):\n",
    "            # We get the example text from lorem web and texts have 4 blank spaces at the begining\n",
    "            # with the next line, we delete them\n",
    "            inserted_line = line[4:]\n",
    "            if len(inserted_line) > max_len:\n",
    "                inserted_line = inserted_line[:max_len-1] + \".\"\n",
    "            elif \"\\n\" in inserted_line:  # We use this only for this example. We remove the \\n from the end of the list.\n",
    "                inserted_line = inserted_line[:len(inserted_line)-1]\n",
    "            # We get the reversed line in list format\n",
    "            reversed_line = ' '.join(inserted_line.split()[::-1])\n",
    "            x.append(list(inserted_line))\n",
    "            # We encode x elements. Now on, every insertion will be encoded\n",
    "            for i, letter in enumerate(x[-1]):\n",
    "                x[-1][i] = char2index[letter]\n",
    "            y = []\n",
    "            # This is to know where to start inserting\n",
    "            start = len(x) - 1\n",
    "            for j in range(len(reversed_line)-1):\n",
    "                anterior = cp.deepcopy(x[start+j])\n",
    "                anterior.append(char2index[reversed_line[j]])\n",
    "                x.append(anterior)\n",
    "                y.append(char2index[reversed_line[j]])\n",
    "            y.append(char2index[reversed_line[-1]])\n",
    "            final_y += y\n",
    "            count += 1\n",
    "\n",
    "    # We fill with \"Null\" values, we do the One-hot encoding and we return the values\n",
    "    x = fill_with(x, char2index[\"Null\"])\n",
    "    oh_x, oh_y = one_hot_encode(x, final_y, tokens)\n",
    "    return oh_x, oh_y, tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reverse_by_words_dataset():\n",
    "    fail = open(\"Files/lorem2.txt\", \"r+\")\n",
    "    list_lines = fail.readlines()\n",
    "    # max_len = len(max(list_lines, key=len))\n",
    "    fail.seek(0)\n",
    "    count = 0\n",
    "    for line in list_lines:\n",
    "        if (line != \"\\n\") and (count < 80):\n",
    "            count += 1\n",
    "            # We get the example text from lorem web and texts have 4 blank spaces at the begining\n",
    "            # with the next line, we delete them\n",
    "            inserted_line = line[4:]\n",
    "            if len(inserted_line) > 40:\n",
    "                # We want our sentences with len=40\n",
    "                inserted_line = inserted_line[:39] + \".\"\n",
    "            # while len(inserted_line) < max_len:\n",
    "            #     inserted_line = \"0\" + inserted_line\n",
    "            fail.write(inserted_line[:len(inserted_line)] +\n",
    "                       \",\" +\n",
    "                       \" \".join(inserted_line.split()[::-1]) +\n",
    "                       \"\\n\")\n",
    "    fail.truncate()\n",
    "    fail.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reverse_dataset():\n",
    "    fail = open(\"Files/lorem.txt\", \"r+\")\n",
    "    list_lines = fail.readlines()\n",
    "    # max_len = len(max(list_lines, key=len))\n",
    "    fail.seek(0)\n",
    "    count = 0\n",
    "    for line in list_lines:\n",
    "        if (line != \"\\n\") and (count < 80):\n",
    "            count += 1\n",
    "            # We get the example text from lorem web and texts have 4 blank spaces at the begining\n",
    "            # with the next line, we delete them\n",
    "            inserted_line = line[4:]\n",
    "            if len(inserted_line) > 40:\n",
    "                # We want our sentences with len=40\n",
    "                inserted_line = inserted_line[:39] + \".\"\n",
    "            # while len(inserted_line) < max_len:\n",
    "            #     inserted_line = \"0\" + inserted_line\n",
    "            fail.write(inserted_line[:len(inserted_line)] +\n",
    "                       \",\" +\n",
    "                       inserted_line[len(inserted_line)-2::-1] +\n",
    "                       \"\\n\")\n",
    "    fail.truncate()\n",
    "    fail.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from create_dataset import create_dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "def run_lstm(maxlen=50, parameters=100, units=32):\n",
    "\n",
    "    x_train, y_train, tokens = create_dataset(maxlen, parameters, 1)\n",
    "    # x.size = (parameters*maxlen, 2*maxlen-1, len(tokens))\n",
    "    # y.size = (parameters*maxlen, len(tokens))\n",
    "    x_test, y_test, tok = create_dataset(maxlen, parameters//3, 2)\n",
    "    model = Sequential()\n",
    "    # model.add(Embedding(len(tokens)+1, 32))  # , input_length=2*maxlen-1))  # imput length se puede quitar en teoria\n",
    "    model.add(LSTM(units, input_shape=(2*maxlen-1, len(tokens))))\n",
    "    model.add(Dense(units))\n",
    "    model.add(Activation('softmax'))\n",
    "    # optimizer = RMSprop(lr=0.01)\n",
    "    # model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    model.fit(x_train, y_test, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
