{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTO 1\n",
    "\n",
    "* Métrica Perplejidad: https://en.wikipedia.org/wiki/Perplexity#cite_ref-1\n",
    "\n",
    "* Loss: Categorical Crossentropy (buscar más referencias para ambos)\n",
    "\n",
    "* Arquitectura: Capa average, capa max y capa densa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@InProceedings{Danescu-Niculescu-Mizil+Lee:11a,\n",
    "\n",
    "  author={Cristian Danescu-Niculescu-Mizil and Lillian Lee},\n",
    "\n",
    "  title={Chameleons in imagined conversations:\n",
    "\n",
    "  A new approach to understanding coordination of linguistic style in dialogs.},\n",
    "\n",
    "  booktitle={Proceedings of the\n",
    "\n",
    "        Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011},\n",
    "\n",
    "  year={2011}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Falta el corpus y hacer train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-318e5da11943>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import average, Dense, maximum\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.utils as ku \n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLSTM(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, tokenizer=Tokenizer()):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def etl(self, data):\n",
    "\n",
    "        # basic cleanup\n",
    "        corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "        # tokenization\n",
    "        self.tokenizer.fit_on_texts(corpus)\n",
    "        self.total_words = len(self.tokenizer.word_index) + 1\n",
    "\n",
    "        # create input sequences using list of tokens\n",
    "        input_sequences = []\n",
    "        for line in corpus:\n",
    "            # TODO: Probar con fastText y HashingVectorizer y los demás de text de keras.\n",
    "            # Onehot no hace falta si usamos embedding\n",
    "            token_list = self.tokenizer.texts_to_sequences([line])[0]\n",
    "            for i in range(1, len(token_list)):\n",
    "                n_gram_sequence = token_list[: i + 1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "\n",
    "        # pad sequences\n",
    "        self.max_sequence_len = max([len(x) for x in input_sequences])\n",
    "        input_sequences = np.array(\n",
    "            pad_sequences(input_sequences, maxlen=self.max_sequence_len, padding=\"pre\")\n",
    "        )\n",
    "        # create X and y\n",
    "        X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "        y = ku.to_categorical(y, num_classes=self.total_words)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def fit(self, X, y, earlystop=False, epochs=200, batch_size=None, verbose=1, activation=\"softmax\", optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]):\n",
    "\n",
    "        self.net = Sequential()\n",
    "        # Hace la media y el máximo respectivamente de dos capas (densas por ejemplo) Como usarlo?\n",
    "        self.net.add(average())\n",
    "        self.net.add(maximum())\n",
    "        self.net.add(Dense(units=self.tokenizer.total_words, activation=activation))\n",
    "\n",
    "        self.net.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=metrics\n",
    "        )\n",
    "        if earlystop:\n",
    "            earlystop = EarlyStopping(\n",
    "                monitor=\"val_loss\", min_delta=0, patience=5, verbose=0, mode=\"auto\"\n",
    "            )\n",
    "            self.net.fit(X, y, epochs=epochs, batch_size=None, verbose=verbose, callbacks=[earlystop])\n",
    "        else:\n",
    "            self.net.fit(X, y, epochs=epochs, batch_size=None, verbose=verbose)\n",
    "        print(self.net.summary())\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def generate_text(self, seed_text, next_words):\n",
    "        \n",
    "        for _ in range(next_words):\n",
    "            token_list = self.tokenizer.texts_to_sequences([seed_text])[0]\n",
    "            token_list = pad_sequences(\n",
    "                [token_list], maxlen=self.max_sequence_len - 1, padding=\"pre\"\n",
    "            )\n",
    "            predicted = self.net.predict_classes(token_list, verbose=0)\n",
    "\n",
    "            output_word = \"\"\n",
    "            for word, index in self.tokenizer.word_index.items():\n",
    "                if index == predicted:\n",
    "                    output_word = word\n",
    "                    break\n",
    "            seed_text += \" \" + output_word\n",
    "        return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = \"\"\"Con diez cañones por banda,\n",
    "viento en popa a toda vela,\n",
    "no corta el mar, sino vuela,\n",
    "un velero bergantín;\n",
    "bajel pirata que llaman\n",
    "por su bravura el Temido\n",
    "en todo el mar conocido\n",
    "del uno al otro confín.\n",
    "La luna en el mar riela,\n",
    "en la lona gime el viento\n",
    "y alza en blando movimiento\n",
    "olas de plata y azul;\n",
    "y ve el capitán pirata,\n",
    "cantando alegre en la popa,\n",
    "Asia a un lado, al otro Europa,\n",
    "Y allá a su frente Estambul:\n",
    "-Navega, velero mío,\n",
    "sin temor\n",
    "que ni enemigo navío,\n",
    "ni tormenta, ni bonanza\n",
    "tu rumbo a torcer alcanza,\n",
    "ni a sujetar tu valor.\n",
    "Veinte presas\n",
    "hemos hecho\n",
    "a despecho\n",
    "del inglés\n",
    "y han rendido\n",
    "sus pendones\n",
    "cien naciones\n",
    "a mis pies.\n",
    "Que es mi barco mi tesoro,\n",
    "que es mi Dios la libertad;\n",
    "mi ley, la fuerza y el viento;\n",
    "mi única patria, la mar.\n",
    "Allá muevan feroz guerra\n",
    "ciegos reyes\n",
    "por un palmo más de tierra,\n",
    "que yo tengo aquí por mío\n",
    "cuanto abarca el mar bravío\n",
    "a quien nadie impuso leyes.\n",
    "Y no hay playa\n",
    "sea cualquiera,\n",
    "ni bandera\n",
    "de esplendor,\n",
    "que no sienta\n",
    "mi derecho\n",
    "y dé pecho\n",
    "a mi valor\n",
    "Que es mi barco mi tesoro,\n",
    "que es mi Dios la libertad;\n",
    "mi ley, la fuerza y el viento;\n",
    "mi única patria, la mar.\n",
    "A la voz de ¡barco viene!,\n",
    "es de ver\n",
    "cómo vira y se previene\n",
    "a todo trapo a escapar:\n",
    "que yo soy el rey del mar\n",
    "y mi furia es de temer.\n",
    "En las presas\n",
    "yo divido\n",
    "lo cogido\n",
    "por igual:\n",
    "sólo quiero\n",
    "por riqueza\n",
    "la belleza\n",
    "sin rival.\n",
    "Que es mi barco mi tesoro,\n",
    "que es mi Dios la libertad;\n",
    "mi ley, la fuerza y el viento;\n",
    "mi única patria, la mar.\n",
    "¡Sentenciado estoy a muerte!\n",
    "Yo me río:\n",
    "no me abandone la suerte,\n",
    "y al mismo que me condena\n",
    "colgaré de alguna antena\n",
    "quizá en su propio navío.\n",
    "Y si caigo,\n",
    "¿qué es la vida?\n",
    "Por perdida\n",
    "ya la di\n",
    "cuando el yugo\n",
    "del esclavo\n",
    "como un bravo sacudí.\n",
    "Que es mi barco mi tesoro,\n",
    "que es mi Dios la libertad;\n",
    "mi ley, la fuerza y el viento;\n",
    "mi única patria, la mar.\n",
    "Son mi música mejor\n",
    "aquilones,\n",
    "el estrépito y temblor\n",
    "de los cables sacudidos\n",
    "del negro mar los bramidos\n",
    "y el rugir de mis cañones.\n",
    "Y del trueno\n",
    "al son violento,\n",
    "y del viento,\n",
    "al rebramar,\n",
    "yo me duermo\n",
    "sosegado,\n",
    "arrullado\n",
    "por el mar.\n",
    "Que es mi barco mi tesoro,\n",
    "que es mi Dios la libertad;\n",
    "mi ley, la fuerza y el viento;\n",
    "mi única patria, la mar.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "average() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d3d78579c71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-69b4adf6949a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, earlystop, epochs, batch_size, verbose, activation, optimizer, loss, metrics)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: average() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "model = TextLSTM()\n",
    "X, y = model.etl(data)\n",
    "model.fit(X, y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
